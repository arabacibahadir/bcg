{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the segmented data folder\n",
    "segmented_data_folder = 'segmented_data'\n",
    "\n",
    "# Initialize lists to hold data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Counters to keep track of how many 'H' and 'N' files are loaded\n",
    "h_count = 0\n",
    "n_count = 0\n",
    "\n",
    "# Define how many of each type to load\n",
    "max_h_files = 30\n",
    "max_n_files = 30\n",
    "\n",
    "# Load the segmented CSV files\n",
    "for file_name in os.listdir(segmented_data_folder):\n",
    "    if h_count >= max_h_files and n_count >= max_n_files:\n",
    "        break\n",
    "    \n",
    "    file_path = os.path.join(segmented_data_folder, file_name)\n",
    "    \n",
    "    # Determine the label from the filename ('H' for hypertensive, 'N' for normal)\n",
    "    if 'H' in file_name and h_count < max_h_files:\n",
    "        label = 1\n",
    "        h_count += 1\n",
    "    elif 'N' in file_name and n_count < max_n_files:\n",
    "        label = 0\n",
    "        n_count += 1\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Read the CSV file\n",
    "    segment = pd.read_csv(file_path)\n",
    "    \n",
    "    # Append the segment data and label\n",
    "    data.append(segment.values)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ],
   "id": "697fb9fffac99e03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# Build an autoencoder model\n",
    "def build_autoencoder(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "    model.add(RepeatVector(input_shape[0]))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(input_shape[1])))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train the autoencoder\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Segment length and number of features\n",
    "autoencoder = build_autoencoder(input_shape)\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=128, validation_data=(X_test, X_test))\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[2].output)\n",
    "encoded_X_train = encoder.predict(X_train)\n",
    "encoded_X_test = encoder.predict(X_test)\n"
   ],
   "id": "48b878e5e11c7149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "22053ed28ca2ae07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build an LSTM model for classification\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(encoded_X_train.shape[1], encoded_X_train.shape[2])))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(encoded_X_train, y_train, epochs=50, batch_size=32, validation_data=(encoded_X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model.evaluate(encoded_X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "id": "cd729f68a272d99f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
